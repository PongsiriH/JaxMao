{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jaxmao/JaxMao/Example',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/jaxmao/.local/lib/python3.10/site-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/jaxmao/JaxMao']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jaxmao/JaxMao\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "import re    \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 42\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "# def generate_dataset(tokens, window_size=5):\n",
    "#     half_window = window_size // 2\n",
    "#     dataset = []\n",
    "\n",
    "#     for i in range(len(tokens)):\n",
    "#         # Extract the window centered at the current token\n",
    "#         start_idx = max(0, i - half_window)\n",
    "#         end_idx = min(len(tokens), i + half_window + 1)\n",
    "#         window = tokens[start_idx:end_idx]\n",
    "\n",
    "#         # If the window size is correct\n",
    "#         if len(window) == window_size:\n",
    "#             input_word = window[half_window]\n",
    "#             output_words = [word for idx, word in enumerate(window) if idx != half_window]\n",
    "#             dataset.append((input_word, output_words))\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "def generate_dataset(tokens, window_size=5):\n",
    "    half_window = window_size // 2\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        # Extract the window centered at the current token\n",
    "        start_idx = max(0, i - half_window)\n",
    "        end_idx = min(len(tokens), i + half_window + 1)\n",
    "        window = tokens[start_idx:end_idx]\n",
    "\n",
    "        # If the window size is correct\n",
    "        if len(window) == window_size:\n",
    "            input_word = window[half_window]\n",
    "            output_words = [word for idx, word in enumerate(window) if idx != half_window]\n",
    "            dataset.append((input_word, output_words))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def generate_dataset2(tokens, encoder, window_size=5):\n",
    "    half_window = window_size // 2\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        # Extract the window centered at the current token\n",
    "        start_idx = max(0, i - half_window)\n",
    "        end_idx = min(len(tokens), i + half_window + 1)\n",
    "        window = tokens[start_idx:end_idx]\n",
    "\n",
    "        # If the window size is correct\n",
    "        if len(window) == window_size:\n",
    "            input_word = encoder([[window[half_window]]])\n",
    "            output_words = [encoder([[word]]) for idx, word in enumerate(window) if idx != half_window]\n",
    "            dataset.append((input_word, output_words))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48611, 1648)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('datasets/randomlyGeneratedText.txt') as f:\n",
    "    sentences = f.read()\n",
    "    sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "    sentences = re.sub(r'\\[ *\\d+ *\\]', ' ', sentences)\n",
    "    sentences = re.sub('ï»¿', '', sentences)\n",
    "    tokens = sentences.lower().split(' ')\n",
    "\n",
    "patterns_to_be_removed = ['\\n', '(', ')']\n",
    "for pattern in patterns_to_be_removed:\n",
    "    tokens = [token.replace(pattern, '') for token in tokens]\n",
    "    \n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "len(tokens), vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder.fit(np.reshape(vocab, (-1,1)))\n",
    "\n",
    "# Kernel keep crashing when use full dataset\n",
    "dataset = generate_dataset2(tokens[:40000], onehot_encoder.transform, window_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[e.toarray() for e in entry[1]] for entry in dataset]).reshape(-1, window_size-1, vocab_size)\n",
    "y = np.array([entry[0].toarray() for entry in dataset]).reshape(-1, vocab_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39996, 4, 1648), (39996, 1648))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('randomlyGeneratedTextX.npy', X)\n",
    "np.save('randomlyGeneratedTexty.npy', y)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['to'],\n",
       "        ['preserved'],\n",
       "        ['mr'],\n",
       "        ['cordially']], dtype='<U14'),\n",
       " array([['be']], dtype='<U14'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder.inverse_transform(X_train[2]), onehot_encoder.inverse_transform(y_train[2].reshape(-1, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from jaxmao.Modules import Module\n",
    "from jaxmao.Layers import FC, SimpleRNN, Flatten\n",
    "from jaxmao.Activations import ReLU, StableSoftmax\n",
    "\n",
    "# Training\n",
    "from jaxmao.Optimizers import GradientDescent, Adam\n",
    "from jaxmao.Losses import CategoricalCrossEntropy\n",
    "\n",
    "class Embedding2(Module):\n",
    "    def __init__(\n",
    "            self\n",
    "        ):\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FC(vocab_size, embedding_dim)\n",
    "        self.fc_out = FC(embedding_dim*4, vocab_size)\n",
    "\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = StableSoftmax()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.softmax(self.fc_out(x))\n",
    "        return x\n",
    "    \n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "embedding_dim = 16\n",
    "PNWmodel = Embedding2()\n",
    "PNWmodel.init_params(key)\n",
    "best_params = {'params' : PNWmodel.params, 'loss' : np.inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_crossentropy = CategoricalCrossEntropy()\n",
    "\n",
    "def loss_params(params, x, y):\n",
    "    pred = PNWmodel.forward(params, x)\n",
    "    return loss_crossentropy(pred, y)\n",
    "\n",
    "grad_loss = jit(value_and_grad(loss_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = GradientDescent()\n",
    "\n",
    "def training_loop(x, y, epochs=20, lr=0.01, batch_size=32, initial_epoch=0):\n",
    "    num_batches = len(x) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        lr = lr*(0.99**initial_epoch)\n",
    "        x, y = shuffle(x, y)\n",
    "        for batch_idx in range(num_batches):\n",
    "            starting_idx = batch_idx * batch_size\n",
    "            ending_idx = (batch_idx + 1) * batch_size\n",
    "            batch_x = x[starting_idx:ending_idx]\n",
    "            batch_y = y[starting_idx:ending_idx]\n",
    "\n",
    "            losses, gradients = grad_loss(PNWmodel.params, batch_x, batch_y)\n",
    "            PNWmodel.params = optimizer(PNWmodel.params, gradients, lr=lr)\n",
    "        \n",
    "        loss = losses / batch_size\n",
    "        if loss < best_params['loss']:\n",
    "            best_params['params'] = PNWmodel.params\n",
    "            best_params['loss'] = loss\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(\"Epoch: {}\\tbatch loss: {}\".format(epoch+1, loss))\n",
    "        \n",
    "        \n",
    "    return PNWmodel.params\n",
    "\n",
    "params = training_loop(\n",
    "        X_train, y_train,\n",
    "        epochs=50, lr=0.01, batch_size=32,\n",
    "        initial_epoch=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.11528338, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1272), (500, 1272))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = PNWmodel.forward(best_params['params'], X_train)\n",
    "output = outputs[:500]\n",
    "trainY = y_train[:500]\n",
    "\n",
    "output.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(output.argmax(axis=1), trainY.argmax(axis=1))\n",
    "print(\"Accuray: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1272), (500, 1272))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = PNWmodel.forward(best_params['params'], X_test)\n",
    "output = outputs[:500]\n",
    "testY = y_test[:500].reshape(-1, vocab_size)\n",
    "\n",
    "output.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray:  0.458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(output.argmax(axis=1), testY.argmax(axis=1))\n",
    "print(\"Accuray: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAXlab",
   "language": "python",
   "name": "jaxlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
