{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAX_CHECK_TRACER_LEAKS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jaxmao/jaxmao_branches/JaxMao/')\n",
    "\n",
    "import jax\n",
    "from jaxmao.modules import Module\n",
    "from jaxmao.layers import Dense, BatchNorm\n",
    "from jaxmao.losses import CategoricalCrossEntropy\n",
    "from jaxmao.optimizers import GradientDescent\n",
    "from jaxmao.utils_struct import _check_dict_ids\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1698329373.362794  115546 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "images, targets = load_digits(return_X_y=True)\n",
    "\n",
    "images = images / images.max()\n",
    "targets_enc = jax.nn.one_hot(targets, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense1': {'weights': Array([[ 0.02117668, -0.22214548, -0.1634809 , ...,  0.02727365,\n",
       "          -0.09970619, -0.13141589],\n",
       "         [ 0.22423503, -0.31929344, -0.05311856, ...,  0.12617505,\n",
       "          -0.05755344, -0.21855643],\n",
       "         [ 0.01177022, -0.06092661, -0.0791641 , ..., -0.08742873,\n",
       "           0.15001932, -0.03505825],\n",
       "         ...,\n",
       "         [-0.1453791 , -0.01812402,  0.28515035, ..., -0.08504849,\n",
       "           0.1471512 , -0.2206304 ],\n",
       "         [-0.02522955, -0.00388724,  0.03972574, ...,  0.20668949,\n",
       "           0.04916492, -0.22023007],\n",
       "         [ 0.17874895,  0.0127775 ,  0.17311396, ...,  0.00680099,\n",
       "          -0.1472737 , -0.14495818]], dtype=float32),\n",
       "  'biases': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)},\n",
       " 'dense2': {'weights': Array([[ 0.09024656,  0.11920545, -0.17902997, -0.15849029,  0.05521125,\n",
       "          -0.37190118,  0.696331  , -0.41349685, -0.15566787,  0.37174168],\n",
       "         [ 0.06852563, -0.12898639, -0.13148971,  0.19048136, -0.15971126,\n",
       "           0.03790694,  0.13622692, -0.35450315, -0.2693221 ,  0.17248774],\n",
       "         [-0.2559495 ,  0.21687885, -0.02222778,  0.31088513,  0.06246506,\n",
       "           0.02807408, -0.2771245 , -0.12836203, -0.10541348,  0.16658928],\n",
       "         [ 0.79426914,  0.3513766 , -0.42071056,  0.11615712, -0.25737846,\n",
       "          -0.00326737, -0.22381218, -0.14644355,  0.17568351,  0.17289135],\n",
       "         [ 0.50836056,  0.06213817,  0.23050962, -0.1185824 ,  0.13404359,\n",
       "          -0.14918548, -0.01823816,  0.18160875,  0.19382031,  0.2285825 ],\n",
       "         [ 0.25171128, -0.2702399 , -0.0069935 , -0.139152  , -0.14551954,\n",
       "           0.34359956, -0.07691216,  0.16400836, -0.17460223, -0.49980953],\n",
       "         [-0.03642946, -0.04636891,  0.33883628, -0.11824276, -0.02391586,\n",
       "           0.40170383,  0.27795002, -0.07773262,  0.15034719, -0.31613344],\n",
       "         [-0.19042082,  0.15911956, -0.08844092,  0.1413988 , -0.1891974 ,\n",
       "          -0.06020748,  0.2923679 ,  0.23401828,  0.56399983,  0.28322405],\n",
       "         [ 0.01763828, -0.08157862, -0.08517836, -0.3925308 , -0.19121782,\n",
       "           0.0498823 ,  0.07808446,  0.06084761,  0.11449283, -0.21881963],\n",
       "         [-0.01112631,  0.10572386,  0.10008298,  0.35875848,  0.1499503 ,\n",
       "          -0.02025919,  0.2183432 ,  0.1721593 , -0.07272509,  0.20259462],\n",
       "         [-0.11438695, -0.2973583 , -0.18915862,  0.5361891 , -0.2147144 ,\n",
       "          -0.15465194,  0.0836774 ,  0.10662986, -0.65367347,  0.29024693],\n",
       "         [ 0.5477235 ,  0.02903549,  0.05425939,  0.41456503,  0.5711089 ,\n",
       "           0.24094155, -0.09014812, -0.2390966 , -0.08589399, -0.07304671],\n",
       "         [-0.58977556, -0.49678198, -0.13707788,  0.15063989, -0.37744156,\n",
       "           0.16080065,  0.0502037 , -0.46545178,  0.14916855, -0.20564635],\n",
       "         [-0.3199451 , -0.0760099 , -0.19308057,  0.5826492 ,  0.02267079,\n",
       "           0.5362233 , -0.12841769,  0.11960324, -0.09215621,  0.0927949 ],\n",
       "         [ 0.5199654 , -0.278645  ,  0.44627708,  0.33236182,  0.5844003 ,\n",
       "          -0.01018172,  0.07246426, -0.22031967, -0.13890846, -0.09873632],\n",
       "         [ 0.16389684, -0.11014237, -0.1716671 ,  0.0773311 , -0.09094501,\n",
       "           0.24319018,  0.09828699,  0.05132258, -0.3147479 ,  0.60429436],\n",
       "         [ 0.17064774,  0.39366812, -0.13582002, -0.38265428,  0.2774833 ,\n",
       "          -0.21578613,  0.12111179,  0.5009843 ,  0.00816146,  0.09218727],\n",
       "         [ 0.14530966,  0.33257326,  0.189043  , -0.11676399,  0.41218963,\n",
       "           0.2395324 ,  0.23542576, -0.2821559 ,  0.26115644, -0.08097478],\n",
       "         [-0.04761033, -0.18725985,  0.19643801,  0.18609805, -0.52199346,\n",
       "           0.04659325,  0.30950853,  0.01595373, -0.2459481 , -0.37143835],\n",
       "         [-0.15275976,  0.24429949, -0.1506858 , -0.31078437,  0.6366616 ,\n",
       "          -0.2125947 , -0.03021047,  0.4723494 , -0.5165172 ,  0.15557359],\n",
       "         [-0.09343935, -0.06939588,  0.44988245,  0.28907833,  0.21421774,\n",
       "          -0.12528056,  0.22090411, -0.39676553, -0.0906697 ,  0.18689963],\n",
       "         [-0.28157339,  0.01525741, -0.29652902, -0.34676233,  0.24498391,\n",
       "          -0.06089066, -0.03471071, -0.26054084,  0.12239934, -0.6137941 ],\n",
       "         [-0.19273518, -0.39644265, -0.00471158, -0.18614255, -0.18289892,\n",
       "           0.11223922,  0.6351809 , -0.1055233 , -0.56767744, -0.04306855],\n",
       "         [-0.18447728, -0.1620116 , -0.13654833,  0.11207812,  0.23757683,\n",
       "          -0.16790979, -0.16556963,  0.20070015,  0.15875894,  0.13137688],\n",
       "         [ 0.27558032, -0.28046766,  0.02011953,  0.38042158, -0.09674725,\n",
       "           0.11673889, -0.5753742 ,  0.36380586,  0.26720983, -0.05372876],\n",
       "         [-0.261188  ,  0.22567703,  0.03290832, -0.16826142,  0.06427699,\n",
       "          -0.09861842,  0.15240863,  0.51589155,  0.12771544,  0.42025584],\n",
       "         [ 0.31126845, -0.08613335,  0.6481123 ,  0.21538106,  0.1173785 ,\n",
       "          -0.05087898, -0.06032199, -0.0622292 , -0.21833527, -0.0858947 ],\n",
       "         [ 0.28436932,  0.1544959 , -0.08637366,  0.32466424, -0.05574786,\n",
       "          -0.0893919 , -0.03034005, -0.02199694, -0.13293989,  0.1199482 ],\n",
       "         [-0.15832949,  0.09027126, -0.42989162, -0.01686097,  0.12571241,\n",
       "           0.15895328, -0.31230682,  0.24718612,  0.08189923,  0.20916536],\n",
       "         [-0.1650787 , -0.11141733,  0.3834728 ,  0.0487185 , -0.07716965,\n",
       "          -0.1182867 ,  0.01642527,  0.02735717, -0.28640354, -0.00184256],\n",
       "         [-0.20011406,  0.12500302,  0.01194058,  0.19129826, -0.01028901,\n",
       "           0.6689401 ,  0.09072208, -0.06972793, -0.21812677, -0.166888  ],\n",
       "         [ 0.12084009,  0.20897092,  0.13673556, -0.32487032, -0.1485783 ,\n",
       "          -0.5973674 ,  0.3415309 , -0.48579302, -0.01720593, -0.47060105]],      dtype=float32),\n",
       "  'biases': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)},\n",
       " 'bn1': {'gamma': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],      dtype=float32),\n",
       "  'beta': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)},\n",
       " 'bn2': {'gamma': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       "  'beta': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(64, 32)\n",
    "        self.dense2 = Dense(32, 10)\n",
    "        self.bn1 = BatchNorm(32)\n",
    "        self.bn2 = BatchNorm(10)\n",
    "        self.bn1.training = True\n",
    "        self.bn2.training = True\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = jax.nn.relu(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = jax.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "seed = 42\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "clf = Classifier()\n",
    "clf.init_params(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Leaked trace MainTrace(2,JVPTrace). Leaked tracer(s):\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902368> is referred to by <dict 140492926503040>['beta']\n<dict 140492926503040> is referred to by <BatchNorm 140497715956496>.params\n<BatchNorm 140497715956496> is referred to by <Classifier 140497878812880>.bn1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902448> is referred to by <dict 140492926503040>['gamma']\n<dict 140492926503040> is referred to by <BatchNorm 140497715956496>.params\n<BatchNorm 140497715956496> is referred to by <Classifier 140497878812880>.bn1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902928> is referred to by <dict 140492926503168>['beta']\n<dict 140492926503168> is referred to by <BatchNorm 140497878799760>.params\n<BatchNorm 140497878799760> is referred to by <Classifier 140497878812880>.bn2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927903408> is referred to by <dict 140492926503168>['gamma']\n<dict 140492926503168> is referred to by <BatchNorm 140497878799760>.params\n<BatchNorm 140497878799760> is referred to by <Classifier 140497878812880>.bn2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902048> is referred to by <dict 140492926503680>['biases']\n<dict 140492926503680> is referred to by <Dense 140497878545232>.params\n<Dense 140497878545232> is referred to by <Classifier 140497878812880>.dense1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([[ 0.02117668 -0.22214548 -0.1634809  ...  0.02727365 -0.09970619\n  -0.13141589]\n [ 0.22423503 -0.31929344 -0.05311856 ...  0.12617505 -0.05755344\n  -0.21855643]\n [ 0.01177022 -0.06092661 -0.0791641  ... -0.08742873  0.15001932\n  -0.03505825]\n ...\n [-0.1453791  -0.01812402  0.28515035 ... -0.08504849  0.1471512\n  -0.2206304 ]\n [-0.02522955 -0.00388724  0.03972574 ...  0.20668949  0.04916492\n  -0.22023007]\n [ 0.17874895  0.0127775   0.17311396 ...  0.00680099 -0.1472737\n  -0.14495818]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[ 0.02117668, -0.22214548, -0.1634809 , ...,  0.02727365,\n        -0.09970619, -0.13141589],\n       [ 0.22423503, -0.31929344, -0.05311856, ...,  0.12617505,\n        -0.05755344, -0.21855643],\n       [ 0.01177022, -0.06092661, -0.0791641 , ..., -0.08742873,\n         0.15001932, -0.03505825],\n       ...,\n       [-0.1453791 , -0.01812402,  0.28515035, ..., -0.08504849,\n         0.1471512 , -0.2206304 ],\n       [-0.02522955, -0.00388724,  0.03972574, ...,  0.20668949,\n         0.04916492, -0.22023007],\n       [ 0.17874895,  0.0127775 ,  0.17311396, ...,  0.00680099,\n        -0.1472737 , -0.14495818]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[64,32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[64,32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902528> is referred to by <dict 140492926503680>['weights']\n<dict 140492926503680> is referred to by <Dense 140497878545232>.params\n<Dense 140497878545232> is referred to by <Classifier 140497878812880>.dense1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927898768> is referred to by <dict 140492926503488>['biases']\n<dict 140492926503488> is referred to by <Dense 140492927090768>.params\n<Dense 140492927090768> is referred to by <Classifier 140497878812880>.dense2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([[ 0.09024656  0.11920545 -0.17902997 -0.15849029  0.05521125 -0.37190118\n   0.696331   -0.41349685 -0.15566787  0.37174168]\n [ 0.06852563 -0.12898639 -0.13148971  0.19048136 -0.15971126  0.03790694\n   0.13622692 -0.35450315 -0.2693221   0.17248774]\n [-0.2559495   0.21687885 -0.02222778  0.31088513  0.06246506  0.02807408\n  -0.2771245  -0.12836203 -0.10541348  0.16658928]\n [ 0.79426914  0.3513766  -0.42071056  0.11615712 -0.25737846 -0.00326737\n  -0.22381218 -0.14644355  0.17568351  0.17289135]\n [ 0.50836056  0.06213817  0.23050962 -0.1185824   0.13404359 -0.14918548\n  -0.01823816  0.18160875  0.19382031  0.2285825 ]\n [ 0.25171128 -0.2702399  -0.0069935  -0.139152   -0.14551954  0.34359956\n  -0.07691216  0.16400836 -0.17460223 -0.49980953]\n [-0.03642946 -0.04636891  0.33883628 -0.11824276 -0.02391586  0.40170383\n   0.27795002 -0.07773262  0.15034719 -0.31613344]\n [-0.19042082  0.15911956 -0.08844092  0.1413988  -0.1891974  -0.06020748\n   0.2923679   0.23401828  0.56399983  0.28322405]\n [ 0.01763828 -0.08157862 -0.08517836 -0.3925308  -0.19121782  0.0498823\n   0.07808446  0.06084761  0.11449283 -0.21881963]\n [-0.01112631  0.10572386  0.10008298  0.35875848  0.1499503  -0.02025919\n   0.2183432   0.1721593  -0.07272509  0.20259462]\n [-0.11438695 -0.2973583  -0.18915862  0.5361891  -0.2147144  -0.15465194\n   0.0836774   0.10662986 -0.65367347  0.29024693]\n [ 0.5477235   0.02903549  0.05425939  0.41456503  0.5711089   0.24094155\n  -0.09014812 -0.2390966  -0.08589399 -0.07304671]\n [-0.58977556 -0.49678198 -0.13707788  0.15063989 -0.37744156  0.16080065\n   0.0502037  -0.46545178  0.14916855 -0.20564635]\n [-0.3199451  -0.0760099  -0.19308057  0.5826492   0.02267079  0.5362233\n  -0.12841769  0.11960324 -0.09215621  0.0927949 ]\n [ 0.5199654  -0.278645    0.44627708  0.33236182  0.5844003  -0.01018172\n   0.07246426 -0.22031967 -0.13890846 -0.09873632]\n [ 0.16389684 -0.11014237 -0.1716671   0.0773311  -0.09094501  0.24319018\n   0.09828699  0.05132258 -0.3147479   0.60429436]\n [ 0.17064774  0.39366812 -0.13582002 -0.38265428  0.2774833  -0.21578613\n   0.12111179  0.5009843   0.00816146  0.09218727]\n [ 0.14530966  0.33257326  0.189043   -0.11676399  0.41218963  0.2395324\n   0.23542576 -0.2821559   0.26115644 -0.08097478]\n [-0.04761033 -0.18725985  0.19643801  0.18609805 -0.52199346  0.04659325\n   0.30950853  0.01595373 -0.2459481  -0.37143835]\n [-0.15275976  0.24429949 -0.1506858  -0.31078437  0.6366616  -0.2125947\n  -0.03021047  0.4723494  -0.5165172   0.15557359]\n [-0.09343935 -0.06939588  0.44988245  0.28907833  0.21421774 -0.12528056\n   0.22090411 -0.39676553 -0.0906697   0.18689963]\n [-0.28157339  0.01525741 -0.29652902 -0.34676233  0.24498391 -0.06089066\n  -0.03471071 -0.26054084  0.12239934 -0.6137941 ]\n [-0.19273518 -0.39644265 -0.00471158 -0.18614255 -0.18289892  0.11223922\n   0.6351809  -0.1055233  -0.56767744 -0.04306855]\n [-0.18447728 -0.1620116  -0.13654833  0.11207812  0.23757683 -0.16790979\n  -0.16556963  0.20070015  0.15875894  0.13137688]\n [ 0.27558032 -0.28046766  0.02011953  0.38042158 -0.09674725  0.11673889\n  -0.5753742   0.36380586  0.26720983 -0.05372876]\n [-0.261188    0.22567703  0.03290832 -0.16826142  0.06427699 -0.09861842\n   0.15240863  0.51589155  0.12771544  0.42025584]\n [ 0.31126845 -0.08613335  0.6481123   0.21538106  0.1173785  -0.05087898\n  -0.06032199 -0.0622292  -0.21833527 -0.0858947 ]\n [ 0.28436932  0.1544959  -0.08637366  0.32466424 -0.05574786 -0.0893919\n  -0.03034005 -0.02199694 -0.13293989  0.1199482 ]\n [-0.15832949  0.09027126 -0.42989162 -0.01686097  0.12571241  0.15895328\n  -0.31230682  0.24718612  0.08189923  0.20916536]\n [-0.1650787  -0.11141733  0.3834728   0.0487185  -0.07716965 -0.1182867\n   0.01642527  0.02735717 -0.28640354 -0.00184256]\n [-0.20011406  0.12500302  0.01194058  0.19129826 -0.01028901  0.6689401\n   0.09072208 -0.06972793 -0.21812677 -0.166888  ]\n [ 0.12084009  0.20897092  0.13673556 -0.32487032 -0.1485783  -0.5973674\n   0.3415309  -0.48579302 -0.01720593 -0.47060105]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[ 0.09024656,  0.11920545, -0.17902997, -0.15849029,  0.05521125,\n        -0.37190118,  0.696331  , -0.41349685, -0.15566787,  0.37174168],\n       [ 0.06852563, -0.12898639, -0.13148971,  0.19048136, -0.15971126,\n         0.03790694,  0.13622692, -0.35450315, -0.2693221 ,  0.17248774],\n       [-0.2559495 ,  0.21687885, -0.02222778,  0.31088513,  0.06246506,\n         0.02807408, -0.2771245 , -0.12836203, -0.10541348,  0.16658928],\n       [ 0.79426914,  0.3513766 , -0.42071056,  0.11615712, -0.25737846,\n        -0.00326737, -0.22381218, -0.14644355,  0.17568351,  0.17289135],\n       [ 0.50836056,  0.06213817,  0.23050962, -0.1185824 ,  0.13404359,\n        -0.14918548, -0.01823816,  0.18160875,  0.19382031,  0.2285825 ],\n       [ 0.25171128, -0.2702399 , -0.0069935 , -0.139152  , -0.14551954,\n         0.34359956, -0.07691216,  0.16400836, -0.17460223, -0.49980953],\n       [-0.03642946, -0.04636891,  0.33883628, -0.11824276, -0.02391586,\n         0.40170383,  0.27795002, -0.07773262,  0.15034719, -0.31613344],\n       [-0.19042082,  0.15911956, -0.08844092,  0.1413988 , -0.1891974 ,\n        -0.06020748,  0.2923679 ,  0.23401828,  0.56399983,  0.28322405],\n       [ 0.01763828, -0.08157862, -0.08517836, -0.3925308 , -0.19121782,\n         0.0498823 ,  0.07808446,  0.06084761,  0.11449283, -0.21881963],\n       [-0.01112631,  0.10572386,  0.10008298,  0.35875848,  0.1499503 ,\n        -0.02025919,  0.2183432 ,  0.1721593 , -0.07272509,  0.20259462],\n       [-0.11438695, -0.2973583 , -0.18915862,  0.5361891 , -0.2147144 ,\n        -0.15465194,  0.0836774 ,  0.10662986, -0.65367347,  0.29024693],\n       [ 0.5477235 ,  0.02903549,  0.05425939,  0.41456503,  0.5711089 ,\n         0.24094155, -0.09014812, -0.2390966 , -0.08589399, -0.07304671],\n       [-0.58977556, -0.49678198, -0.13707788,  0.15063989, -0.37744156,\n         0.16080065,  0.0502037 , -0.46545178,  0.14916855, -0.20564635],\n       [-0.3199451 , -0.0760099 , -0.19308057,  0.5826492 ,  0.02267079,\n         0.5362233 , -0.12841769,  0.11960324, -0.09215621,  0.0927949 ],\n       [ 0.5199654 , -0.278645  ,  0.44627708,  0.33236182,  0.5844003 ,\n        -0.01018172,  0.07246426, -0.22031967, -0.13890846, -0.09873632],\n       [ 0.16389684, -0.11014237, -0.1716671 ,  0.0773311 , -0.09094501,\n         0.24319018,  0.09828699,  0.05132258, -0.3147479 ,  0.60429436],\n       [ 0.17064774,  0.39366812, -0.13582002, -0.38265428,  0.2774833 ,\n        -0.21578613,  0.12111179,  0.5009843 ,  0.00816146,  0.09218727],\n       [ 0.14530966,  0.33257326,  0.189043  , -0.11676399,  0.41218963,\n         0.2395324 ,  0.23542576, -0.2821559 ,  0.26115644, -0.08097478],\n       [-0.04761033, -0.18725985,  0.19643801,  0.18609805, -0.52199346,\n         0.04659325,  0.30950853,  0.01595373, -0.2459481 , -0.37143835],\n       [-0.15275976,  0.24429949, -0.1506858 , -0.31078437,  0.6366616 ,\n        -0.2125947 , -0.03021047,  0.4723494 , -0.5165172 ,  0.15557359],\n       [-0.09343935, -0.06939588,  0.44988245,  0.28907833,  0.21421774,\n        -0.12528056,  0.22090411, -0.39676553, -0.0906697 ,  0.18689963],\n       [-0.28157339,  0.01525741, -0.29652902, -0.34676233,  0.24498391,\n        -0.06089066, -0.03471071, -0.26054084,  0.12239934, -0.6137941 ],\n       [-0.19273518, -0.39644265, -0.00471158, -0.18614255, -0.18289892,\n         0.11223922,  0.6351809 , -0.1055233 , -0.56767744, -0.04306855],\n       [-0.18447728, -0.1620116 , -0.13654833,  0.11207812,  0.23757683,\n        -0.16790979, -0.16556963,  0.20070015,  0.15875894,  0.13137688],\n       [ 0.27558032, -0.28046766,  0.02011953,  0.38042158, -0.09674725,\n         0.11673889, -0.5753742 ,  0.36380586,  0.26720983, -0.05372876],\n       [-0.261188  ,  0.22567703,  0.03290832, -0.16826142,  0.06427699,\n        -0.09861842,  0.15240863,  0.51589155,  0.12771544,  0.42025584],\n       [ 0.31126845, -0.08613335,  0.6481123 ,  0.21538106,  0.1173785 ,\n        -0.05087898, -0.06032199, -0.0622292 , -0.21833527, -0.0858947 ],\n       [ 0.28436932,  0.1544959 , -0.08637366,  0.32466424, -0.05574786,\n        -0.0893919 , -0.03034005, -0.02199694, -0.13293989,  0.1199482 ],\n       [-0.15832949,  0.09027126, -0.42989162, -0.01686097,  0.12571241,\n         0.15895328, -0.31230682,  0.24718612,  0.08189923,  0.20916536],\n       [-0.1650787 , -0.11141733,  0.3834728 ,  0.0487185 , -0.07716965,\n        -0.1182867 ,  0.01642527,  0.02735717, -0.28640354, -0.00184256],\n       [-0.20011406,  0.12500302,  0.01194058,  0.19129826, -0.01028901,\n         0.6689401 ,  0.09072208, -0.06972793, -0.21812677, -0.166888  ],\n       [ 0.12084009,  0.20897092,  0.13673556, -0.32487032, -0.1485783 ,\n        -0.5973674 ,  0.3415309 , -0.48579302, -0.01720593, -0.47060105]],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32,10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32,10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927898848> is referred to by <dict 140492926503488>['weights']\n<dict 140492926503488> is referred to by <Dense 140492927090768>.params\n<Dense 140492927090768> is referred to by <Classifier 140497878812880>.dense2\n<Classifier 140497878812880> is referred to by __main__.clf\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m total_losses \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(BATCH_SIZE): \n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     loss, gradients \u001b[39m=\u001b[39m loss_and_grad(clf\u001b[39m.\u001b[39;49mparams, images, targets_enc)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     new_params, optimizer\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m optimizer(clf\u001b[39m.\u001b[39mparams, gradients, optimizer\u001b[39m.\u001b[39mstate)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# clf.update_params(new_params)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/jaxmao/jaxmao_branches/JaxMao/Example/DigitClassifier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# clf.update_state()\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxlab/lib/python3.11/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[1;32m    145\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxlab/lib/python3.11/site-packages/jax/_src/core.py:1111\u001b[0m, in \u001b[0;36mnew_main\u001b[0;34m(trace_type, dynamic, **payload)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m t() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1110\u001b[0m   leaked_tracers \u001b[39m=\u001b[39m maybe_find_leaked_tracers(t())\n\u001b[0;32m-> 1111\u001b[0m   \u001b[39mif\u001b[39;00m leaked_tracers: \u001b[39mraise\u001b[39;00m leaked_tracer_error(\u001b[39m\"\u001b[39m\u001b[39mtrace\u001b[39m\u001b[39m\"\u001b[39m, t(), leaked_tracers)\n",
      "\u001b[0;31mException\u001b[0m: Leaked trace MainTrace(2,JVPTrace). Leaked tracer(s):\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902368> is referred to by <dict 140492926503040>['beta']\n<dict 140492926503040> is referred to by <BatchNorm 140497715956496>.params\n<BatchNorm 140497715956496> is referred to by <Classifier 140497878812880>.bn1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902448> is referred to by <dict 140492926503040>['gamma']\n<dict 140492926503040> is referred to by <BatchNorm 140497715956496>.params\n<BatchNorm 140497715956496> is referred to by <Classifier 140497878812880>.bn1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902928> is referred to by <dict 140492926503168>['beta']\n<dict 140492926503168> is referred to by <BatchNorm 140497878799760>.params\n<BatchNorm 140497878799760> is referred to by <Classifier 140497878812880>.bn2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927903408> is referred to by <dict 140492926503168>['gamma']\n<dict 140492926503168> is referred to by <BatchNorm 140497878799760>.params\n<BatchNorm 140497878799760> is referred to by <Classifier 140497878812880>.bn2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902048> is referred to by <dict 140492926503680>['biases']\n<dict 140492926503680> is referred to by <Dense 140497878545232>.params\n<Dense 140497878545232> is referred to by <Classifier 140497878812880>.dense1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([[ 0.02117668 -0.22214548 -0.1634809  ...  0.02727365 -0.09970619\n  -0.13141589]\n [ 0.22423503 -0.31929344 -0.05311856 ...  0.12617505 -0.05755344\n  -0.21855643]\n [ 0.01177022 -0.06092661 -0.0791641  ... -0.08742873  0.15001932\n  -0.03505825]\n ...\n [-0.1453791  -0.01812402  0.28515035 ... -0.08504849  0.1471512\n  -0.2206304 ]\n [-0.02522955 -0.00388724  0.03972574 ...  0.20668949  0.04916492\n  -0.22023007]\n [ 0.17874895  0.0127775   0.17311396 ...  0.00680099 -0.1472737\n  -0.14495818]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[ 0.02117668, -0.22214548, -0.1634809 , ...,  0.02727365,\n        -0.09970619, -0.13141589],\n       [ 0.22423503, -0.31929344, -0.05311856, ...,  0.12617505,\n        -0.05755344, -0.21855643],\n       [ 0.01177022, -0.06092661, -0.0791641 , ..., -0.08742873,\n         0.15001932, -0.03505825],\n       ...,\n       [-0.1453791 , -0.01812402,  0.28515035, ..., -0.08504849,\n         0.1471512 , -0.2206304 ],\n       [-0.02522955, -0.00388724,  0.03972574, ...,  0.20668949,\n         0.04916492, -0.22023007],\n       [ 0.17874895,  0.0127775 ,  0.17311396, ...,  0.00680099,\n        -0.1472737 , -0.14495818]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[64,32])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[64,32]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927902528> is referred to by <dict 140492926503680>['weights']\n<dict 140492926503680> is referred to by <Dense 140497878545232>.params\n<Dense 140497878545232> is referred to by <Classifier 140497878812880>.dense1\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927898768> is referred to by <dict 140492926503488>['biases']\n<dict 140492926503488> is referred to by <Dense 140492927090768>.params\n<Dense 140492927090768> is referred to by <Classifier 140497878812880>.dense2\n<Classifier 140497878812880> is referred to by __main__.clf\n\nTraced<ConcreteArray([[ 0.09024656  0.11920545 -0.17902997 -0.15849029  0.05521125 -0.37190118\n   0.696331   -0.41349685 -0.15566787  0.37174168]\n [ 0.06852563 -0.12898639 -0.13148971  0.19048136 -0.15971126  0.03790694\n   0.13622692 -0.35450315 -0.2693221   0.17248774]\n [-0.2559495   0.21687885 -0.02222778  0.31088513  0.06246506  0.02807408\n  -0.2771245  -0.12836203 -0.10541348  0.16658928]\n [ 0.79426914  0.3513766  -0.42071056  0.11615712 -0.25737846 -0.00326737\n  -0.22381218 -0.14644355  0.17568351  0.17289135]\n [ 0.50836056  0.06213817  0.23050962 -0.1185824   0.13404359 -0.14918548\n  -0.01823816  0.18160875  0.19382031  0.2285825 ]\n [ 0.25171128 -0.2702399  -0.0069935  -0.139152   -0.14551954  0.34359956\n  -0.07691216  0.16400836 -0.17460223 -0.49980953]\n [-0.03642946 -0.04636891  0.33883628 -0.11824276 -0.02391586  0.40170383\n   0.27795002 -0.07773262  0.15034719 -0.31613344]\n [-0.19042082  0.15911956 -0.08844092  0.1413988  -0.1891974  -0.06020748\n   0.2923679   0.23401828  0.56399983  0.28322405]\n [ 0.01763828 -0.08157862 -0.08517836 -0.3925308  -0.19121782  0.0498823\n   0.07808446  0.06084761  0.11449283 -0.21881963]\n [-0.01112631  0.10572386  0.10008298  0.35875848  0.1499503  -0.02025919\n   0.2183432   0.1721593  -0.07272509  0.20259462]\n [-0.11438695 -0.2973583  -0.18915862  0.5361891  -0.2147144  -0.15465194\n   0.0836774   0.10662986 -0.65367347  0.29024693]\n [ 0.5477235   0.02903549  0.05425939  0.41456503  0.5711089   0.24094155\n  -0.09014812 -0.2390966  -0.08589399 -0.07304671]\n [-0.58977556 -0.49678198 -0.13707788  0.15063989 -0.37744156  0.16080065\n   0.0502037  -0.46545178  0.14916855 -0.20564635]\n [-0.3199451  -0.0760099  -0.19308057  0.5826492   0.02267079  0.5362233\n  -0.12841769  0.11960324 -0.09215621  0.0927949 ]\n [ 0.5199654  -0.278645    0.44627708  0.33236182  0.5844003  -0.01018172\n   0.07246426 -0.22031967 -0.13890846 -0.09873632]\n [ 0.16389684 -0.11014237 -0.1716671   0.0773311  -0.09094501  0.24319018\n   0.09828699  0.05132258 -0.3147479   0.60429436]\n [ 0.17064774  0.39366812 -0.13582002 -0.38265428  0.2774833  -0.21578613\n   0.12111179  0.5009843   0.00816146  0.09218727]\n [ 0.14530966  0.33257326  0.189043   -0.11676399  0.41218963  0.2395324\n   0.23542576 -0.2821559   0.26115644 -0.08097478]\n [-0.04761033 -0.18725985  0.19643801  0.18609805 -0.52199346  0.04659325\n   0.30950853  0.01595373 -0.2459481  -0.37143835]\n [-0.15275976  0.24429949 -0.1506858  -0.31078437  0.6366616  -0.2125947\n  -0.03021047  0.4723494  -0.5165172   0.15557359]\n [-0.09343935 -0.06939588  0.44988245  0.28907833  0.21421774 -0.12528056\n   0.22090411 -0.39676553 -0.0906697   0.18689963]\n [-0.28157339  0.01525741 -0.29652902 -0.34676233  0.24498391 -0.06089066\n  -0.03471071 -0.26054084  0.12239934 -0.6137941 ]\n [-0.19273518 -0.39644265 -0.00471158 -0.18614255 -0.18289892  0.11223922\n   0.6351809  -0.1055233  -0.56767744 -0.04306855]\n [-0.18447728 -0.1620116  -0.13654833  0.11207812  0.23757683 -0.16790979\n  -0.16556963  0.20070015  0.15875894  0.13137688]\n [ 0.27558032 -0.28046766  0.02011953  0.38042158 -0.09674725  0.11673889\n  -0.5753742   0.36380586  0.26720983 -0.05372876]\n [-0.261188    0.22567703  0.03290832 -0.16826142  0.06427699 -0.09861842\n   0.15240863  0.51589155  0.12771544  0.42025584]\n [ 0.31126845 -0.08613335  0.6481123   0.21538106  0.1173785  -0.05087898\n  -0.06032199 -0.0622292  -0.21833527 -0.0858947 ]\n [ 0.28436932  0.1544959  -0.08637366  0.32466424 -0.05574786 -0.0893919\n  -0.03034005 -0.02199694 -0.13293989  0.1199482 ]\n [-0.15832949  0.09027126 -0.42989162 -0.01686097  0.12571241  0.15895328\n  -0.31230682  0.24718612  0.08189923  0.20916536]\n [-0.1650787  -0.11141733  0.3834728   0.0487185  -0.07716965 -0.1182867\n   0.01642527  0.02735717 -0.28640354 -0.00184256]\n [-0.20011406  0.12500302  0.01194058  0.19129826 -0.01028901  0.6689401\n   0.09072208 -0.06972793 -0.21812677 -0.166888  ]\n [ 0.12084009  0.20897092  0.13673556 -0.32487032 -0.1485783  -0.5973674\n   0.3415309  -0.48579302 -0.01720593 -0.47060105]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[ 0.09024656,  0.11920545, -0.17902997, -0.15849029,  0.05521125,\n        -0.37190118,  0.696331  , -0.41349685, -0.15566787,  0.37174168],\n       [ 0.06852563, -0.12898639, -0.13148971,  0.19048136, -0.15971126,\n         0.03790694,  0.13622692, -0.35450315, -0.2693221 ,  0.17248774],\n       [-0.2559495 ,  0.21687885, -0.02222778,  0.31088513,  0.06246506,\n         0.02807408, -0.2771245 , -0.12836203, -0.10541348,  0.16658928],\n       [ 0.79426914,  0.3513766 , -0.42071056,  0.11615712, -0.25737846,\n        -0.00326737, -0.22381218, -0.14644355,  0.17568351,  0.17289135],\n       [ 0.50836056,  0.06213817,  0.23050962, -0.1185824 ,  0.13404359,\n        -0.14918548, -0.01823816,  0.18160875,  0.19382031,  0.2285825 ],\n       [ 0.25171128, -0.2702399 , -0.0069935 , -0.139152  , -0.14551954,\n         0.34359956, -0.07691216,  0.16400836, -0.17460223, -0.49980953],\n       [-0.03642946, -0.04636891,  0.33883628, -0.11824276, -0.02391586,\n         0.40170383,  0.27795002, -0.07773262,  0.15034719, -0.31613344],\n       [-0.19042082,  0.15911956, -0.08844092,  0.1413988 , -0.1891974 ,\n        -0.06020748,  0.2923679 ,  0.23401828,  0.56399983,  0.28322405],\n       [ 0.01763828, -0.08157862, -0.08517836, -0.3925308 , -0.19121782,\n         0.0498823 ,  0.07808446,  0.06084761,  0.11449283, -0.21881963],\n       [-0.01112631,  0.10572386,  0.10008298,  0.35875848,  0.1499503 ,\n        -0.02025919,  0.2183432 ,  0.1721593 , -0.07272509,  0.20259462],\n       [-0.11438695, -0.2973583 , -0.18915862,  0.5361891 , -0.2147144 ,\n        -0.15465194,  0.0836774 ,  0.10662986, -0.65367347,  0.29024693],\n       [ 0.5477235 ,  0.02903549,  0.05425939,  0.41456503,  0.5711089 ,\n         0.24094155, -0.09014812, -0.2390966 , -0.08589399, -0.07304671],\n       [-0.58977556, -0.49678198, -0.13707788,  0.15063989, -0.37744156,\n         0.16080065,  0.0502037 , -0.46545178,  0.14916855, -0.20564635],\n       [-0.3199451 , -0.0760099 , -0.19308057,  0.5826492 ,  0.02267079,\n         0.5362233 , -0.12841769,  0.11960324, -0.09215621,  0.0927949 ],\n       [ 0.5199654 , -0.278645  ,  0.44627708,  0.33236182,  0.5844003 ,\n        -0.01018172,  0.07246426, -0.22031967, -0.13890846, -0.09873632],\n       [ 0.16389684, -0.11014237, -0.1716671 ,  0.0773311 , -0.09094501,\n         0.24319018,  0.09828699,  0.05132258, -0.3147479 ,  0.60429436],\n       [ 0.17064774,  0.39366812, -0.13582002, -0.38265428,  0.2774833 ,\n        -0.21578613,  0.12111179,  0.5009843 ,  0.00816146,  0.09218727],\n       [ 0.14530966,  0.33257326,  0.189043  , -0.11676399,  0.41218963,\n         0.2395324 ,  0.23542576, -0.2821559 ,  0.26115644, -0.08097478],\n       [-0.04761033, -0.18725985,  0.19643801,  0.18609805, -0.52199346,\n         0.04659325,  0.30950853,  0.01595373, -0.2459481 , -0.37143835],\n       [-0.15275976,  0.24429949, -0.1506858 , -0.31078437,  0.6366616 ,\n        -0.2125947 , -0.03021047,  0.4723494 , -0.5165172 ,  0.15557359],\n       [-0.09343935, -0.06939588,  0.44988245,  0.28907833,  0.21421774,\n        -0.12528056,  0.22090411, -0.39676553, -0.0906697 ,  0.18689963],\n       [-0.28157339,  0.01525741, -0.29652902, -0.34676233,  0.24498391,\n        -0.06089066, -0.03471071, -0.26054084,  0.12239934, -0.6137941 ],\n       [-0.19273518, -0.39644265, -0.00471158, -0.18614255, -0.18289892,\n         0.11223922,  0.6351809 , -0.1055233 , -0.56767744, -0.04306855],\n       [-0.18447728, -0.1620116 , -0.13654833,  0.11207812,  0.23757683,\n        -0.16790979, -0.16556963,  0.20070015,  0.15875894,  0.13137688],\n       [ 0.27558032, -0.28046766,  0.02011953,  0.38042158, -0.09674725,\n         0.11673889, -0.5753742 ,  0.36380586,  0.26720983, -0.05372876],\n       [-0.261188  ,  0.22567703,  0.03290832, -0.16826142,  0.06427699,\n        -0.09861842,  0.15240863,  0.51589155,  0.12771544,  0.42025584],\n       [ 0.31126845, -0.08613335,  0.6481123 ,  0.21538106,  0.1173785 ,\n        -0.05087898, -0.06032199, -0.0622292 , -0.21833527, -0.0858947 ],\n       [ 0.28436932,  0.1544959 , -0.08637366,  0.32466424, -0.05574786,\n        -0.0893919 , -0.03034005, -0.02199694, -0.13293989,  0.1199482 ],\n       [-0.15832949,  0.09027126, -0.42989162, -0.01686097,  0.12571241,\n         0.15895328, -0.31230682,  0.24718612,  0.08189923,  0.20916536],\n       [-0.1650787 , -0.11141733,  0.3834728 ,  0.0487185 , -0.07716965,\n        -0.1182867 ,  0.01642527,  0.02735717, -0.28640354, -0.00184256],\n       [-0.20011406,  0.12500302,  0.01194058,  0.19129826, -0.01028901,\n         0.6689401 ,  0.09072208, -0.06972793, -0.21812677, -0.166888  ],\n       [ 0.12084009,  0.20897092,  0.13673556, -0.32487032, -0.1485783 ,\n        -0.5973674 ,  0.3415309 , -0.48579302, -0.01720593, -0.47060105]],      dtype=float32)\n  tangent = Traced<ShapedArray(float32[32,10])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[32,10]), None)\n    recipe = LambdaBinding()\n<JVPTracer 140492927898848> is referred to by <dict 140492926503488>['weights']\n<dict 140492926503488> is referred to by <Dense 140492927090768>.params\n<Dense 140492927090768> is referred to by <Classifier 140497878812880>.dense2\n<Classifier 140497878812880> is referred to by __main__.clf\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CategoricalCrossEntropy(reduce_fn='mean_over_batch_size')\n",
    "optimizer = GradientDescent(lr=0.01, params=clf.params)\n",
    "\n",
    "# @jax.jit\n",
    "def loss_fn_wrapped(params, x, y):\n",
    "    y_pred = clf.apply(params, x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss\n",
    "\n",
    "loss_and_grad = jax.value_and_grad(loss_fn_wrapped, argnums=0, has_aux=False)\n",
    "loss, gradients = loss_and_grad(clf.params, images, targets_enc)\n",
    "\n",
    "with jax.check_tracer_leaks() as T:\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 128    \n",
    "    NUM_BATCHES = len(images) // BATCH_SIZE\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_losses = 0.0\n",
    "        for n in range(BATCH_SIZE): \n",
    "            loss, gradients = loss_and_grad(clf.params, images, targets_enc)\n",
    "            new_params, optimizer.state = optimizer(clf.params, gradients, optimizer.state)\n",
    "            # clf.update_params(new_params)\n",
    "            # clf.update_state()\n",
    "            total_losses += loss\n",
    "        print('epoch: {} - avg_loss: {} {}'.format(epoch+1, total_losses/NUM_BATCHES, T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids are consistent. There are no copies of same items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params dense1:  False\n",
      "params dense2:  False\n",
      "state dense1:  True\n",
      "state dense2:  True\n"
     ]
    }
   ],
   "source": [
    "print('params dense1: ', _check_dict_ids(clf.params['dense1'], clf.layers['dense1'].params))\n",
    "print('params dense2: ', _check_dict_ids(clf.params['dense2'], clf.layers['dense2'].params))\n",
    "\n",
    "print('state dense1: ', _check_dict_ids(clf.state['dense1'], clf.layers['dense1'].state))\n",
    "print('state dense2: ', _check_dict_ids(clf.state['dense2'], clf.layers['dense2'].state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equivalent:  False\n",
      "Accuracy : 0.968280\n",
      "Precision: 0.968663\n",
      "Recall   : 0.968199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "y_pred = clf(images)\n",
    "y_pred2 = clf.apply(clf.params, images)\n",
    "print('Equivalent: ', (y_pred==y_pred2).all())\n",
    "\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "accuracy = accuracy_score(targets, y_pred)\n",
    "precision = precision_score(targets, y_pred, average='macro')\n",
    "recall = recall_score(targets, y_pred, average='macro')\n",
    "print('Accuracy : {:<.6f}'.format(accuracy))\n",
    "print('Precision: {:<.6f}'.format(precision))\n",
    "print('Recall   : {:<.6f}'.format(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
