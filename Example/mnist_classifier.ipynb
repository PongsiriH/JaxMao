{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jaxmao/JaxMao/Example', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/jaxmao/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/home/jaxmao/JaxMao']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jaxmao/JaxMao\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 19:51:15.994132: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-13 19:51:16.039941: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-13 19:51:16.041051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-13 19:51:16.909455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras import datasets\n",
    "from keras.utils import to_categorical\n",
    "from jax import random, grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jaxmao.Modules import Module\n",
    "from jaxmao.Layers import FC, Conv2D, Flatten\n",
    "from jaxmao.Activations import ReLU, StableSoftmax\n",
    "\n",
    "seed = 42\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "X_train = jnp.reshape(X_train/255., (-1, 1, 28, 28)).astype(jnp.float32)\n",
    "X_test = jnp.reshape(X_test/255., (-1, 1, 28, 28)).astype(jnp.float32)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten, tree_unflatten, tree_structure\n",
    "\n",
    "\n",
    "class MNIST_Classifier(Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(1, 32, 3)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FC(32*28*28, 32)\n",
    "        self.fc2 = FC(32, 10)\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = StableSoftmax()\n",
    "        \n",
    "        \n",
    "    def _forward(self, params, x):\n",
    "        # for layer, new_params in zip(self.layers, params):\n",
    "        #     layer.params = new_params\n",
    "        # x = self.relu(self.fc1._forward(self.fc1.params, x))\n",
    "        # x = self.relu(self.fc2._forward(self.fc2.params, x))\n",
    "        # x = self.softmax(self.fc3._forward(self.fc3.params, x))\n",
    "        # return x\n",
    "        x = self.conv1(self.conv1._forward(params[0], x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1._forward(params[2], x))\n",
    "        x = self.softmax(self.fc2._forward(params[3], x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, params, x):\n",
    "        # for layer, new_params in zip(self.layers, params):\n",
    "        #     layer.params = new_params\n",
    "        # x = self.relu(self.fc1._forward(self.fc1.params, x))\n",
    "        # x = self.relu(self.fc2._forward(self.fc2.params, x))\n",
    "        # x = self.softmax(self.fc3._forward(self.fc3.params, x))\n",
    "        # return x\n",
    "        \n",
    "        x = self.relu(self.conv1.forward(params[0], x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1.forward(params[2], x))\n",
    "        x = self.softmax(self.fc2.forward(params[3], x))\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "clf = MNIST_Classifier()\n",
    "clf.init_params(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 3, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.params[0]['weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 10),\n",
       " Array([1.        , 1.        , 1.        , 0.9999999 , 1.0000001 ,\n",
       "        1.        , 0.99999994, 1.        , 1.0000001 , 1.0000001 ,\n",
       "        1.        , 0.9999999 , 1.        , 1.        , 1.0000001 ,\n",
       "        1.        , 1.        , 1.        , 0.99999994, 0.99999994],      dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = clf.forward(clf.params, X_train[:20])\n",
    "output.shape, output.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val, structure = tree_flatten(clf.params)\n",
    "# tree_unflatten(structure, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad, jit, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxmao.Optimizers import GradientDescent\n",
    "from jaxmao.Losses import MeanSquaredError\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "loss = MeanSquaredError()\n",
    "optimizer = GradientDescent()\n",
    "\n",
    "def loss_params(params, x, y):\n",
    "    pred = clf.forward(params, x)\n",
    "    return loss(pred, y)\n",
    "\n",
    "grad_loss = jit(value_and_grad(loss_params))\n",
    "\n",
    "# def training_loop(epochs=20, lr=0.01):\n",
    "#     losses, gradients = grad_loss(clf.params, X_train[:50], y_train[:50])\n",
    "#     # return losses, gradients\n",
    "#     clf.params = optimizer(clf.params, gradients, lr=lr)\n",
    "\n",
    "def training_loop(x, y, epochs=20, lr=0.01, batch_size=32):\n",
    "    num_batches = len(x) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x, y = shuffle(x, y)\n",
    "        for batch_idx in range(num_batches):\n",
    "            starting_idx = batch_idx * batch_size\n",
    "            ending_idx = (batch_idx + 1) * batch_size\n",
    "            batch_x = x[starting_idx:ending_idx]\n",
    "            batch_y = y[starting_idx:ending_idx]\n",
    "            \n",
    "            losses, gradients = grad_loss(clf.params, batch_x, batch_y)\n",
    "            clf.params = optimizer(clf.params, gradients, lr=lr)\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"Epoch: {}\\tLoss: {}\".format(epoch+1, loss_params(clf.params, batch_x, batch_y)))\n",
    "    \n",
    "    return clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.forward(clf.params, X_train[:20]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax.tree_util import tree_leaves\n",
    "\n",
    "# s = tree_structure(clf.layers[0].params)\n",
    "# v = tree_leaves(clf.layers[0].params)\n",
    "# tree_unflatten(s, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tLoss: 0.0037350631318986416\n",
      "Epoch: 10\tLoss: 0.0028351321816444397\n",
      "Epoch: 15\tLoss: 0.0029883235692977905\n",
      "Epoch: 20\tLoss: 0.0038615725934505463\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    X_train, y_train, \n",
    "    epochs=21, lr=0.05, batch_size=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 10),\n",
       " Array([3, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 1, 3, 6, 1, 9, 2, 8, 6, 7], dtype=int32),\n",
       " array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9]),\n",
       " Array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0], dtype=int32),\n",
       " Array(16, dtype=int32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = clf.forward(clf.params, X_train[:20])\n",
    "(output.shape, output.argmax(axis=1), \n",
    " y_train[:20].argmax(axis=1), \n",
    " (y_train[:20].argmax(axis=1) == output.argmax(axis=1)).astype(int), \n",
    " (y_train[:20].argmax(axis=1) == output.argmax(axis=1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68325"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(clf(X_train[4000:8000]).argmax(axis=1), y_train[4000:8000].argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(params) == id(clf.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAXlab",
   "language": "python",
   "name": "jaxlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
