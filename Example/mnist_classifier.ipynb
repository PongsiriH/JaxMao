{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add JaxMao 'library' to PYTHONPATH\n",
    "\n",
    "**Will remove this section layer when I know how to add package to actual python's path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jaxmao/JaxMao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "- Import packages\n",
    "- Import JaxMao\n",
    "- Set seed and key\n",
    "- Import and prepare data (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Jax, MNIST dataset, utils functions and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 20:48:03.067235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "from keras import datasets\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 42\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import JaxMao functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from jaxmao.Modules import Module\n",
    "from jaxmao.Layers import FC, Conv2D, Flatten\n",
    "from jaxmao.Activations import ReLU, StableSoftmax\n",
    "\n",
    "# Training\n",
    "from jaxmao.Optimizers import GradientDescent\n",
    "from jaxmao.Losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import and prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "X_train = jnp.array(X_train/255., jnp.float32).reshape(-1, 1, 28, 28)\n",
    "X_test = jnp.array(X_test/255., jnp.float32).reshape(-1, 1, 28, 28)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our MNIST Classifier\n",
    "\n",
    "We will build simple Convolution-FC-FC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Classifier(Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(1, 32, 3, 2) \n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FC(32*14*14, 32)\n",
    "        self.fc2 = FC(32, 10)\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = StableSoftmax()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model and analyze output behavior\n",
    "- output behavior: shape, value, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "clf = MNIST_Classifier()\n",
    "clf.init_params(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict without and training:** Let see what our model's output look like <br>\n",
    "- The output shape should be (num_inputs, num_classes), where num_classes = 10.\n",
    "- Since our last layer is softmax, sum of each output should equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  (20, 10)\n",
      "\n",
      "output sum:\n",
      " [1.         1.0000001  1.         0.99999994 1.         0.9999999\n",
      " 1.         1.         1.0000001  1.         0.99999994 1.0000001\n",
      " 1.0000001  0.99999994 1.         1.         1.0000001  0.99999994\n",
      " 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = X_train[:20]\n",
    "output = clf(x)\n",
    "print(\"output shape: \", output.shape)\n",
    "print()\n",
    "print(\"output sum:\\n\", output.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and jax.grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MeanSquaredError()\n",
    "\n",
    "def loss_params(params, x, y):\n",
    "    pred = clf.forward(params, x)\n",
    "    return loss(pred, y)\n",
    "\n",
    "grad_loss = jit(value_and_grad(loss_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer and training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent on 50 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescent()\n",
    "\n",
    "def training_loop(epochs=20, lr=0.01):\n",
    "    losses, gradients = grad_loss(clf.params, X_train[:50], y_train[:50])\n",
    "    clf.params = optimizer(clf.params, gradients, lr=lr)\n",
    "    return clf.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent training loop\n",
    "Usually, taking gradient on all data points is not possible. <br>\n",
    "We cannot put our entire data into the memory all at once. <br> \n",
    "<br>\n",
    "**Stochastic Gradient Descent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(x, y, epochs=20, lr=0.01, batch_size=32):\n",
    "    num_batches = len(x) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x, y = shuffle(x, y)\n",
    "        for batch_idx in range(num_batches):\n",
    "            starting_idx = batch_idx * batch_size\n",
    "            ending_idx = (batch_idx + 1) * batch_size\n",
    "            batch_x = x[starting_idx:ending_idx]\n",
    "            batch_y = y[starting_idx:ending_idx]\n",
    "            \n",
    "            losses, gradients = grad_loss(clf.params, batch_x, batch_y)\n",
    "            clf.params = optimizer(clf.params, gradients, lr=lr)\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"Epoch: {}\\tLoss: {}\".format(epoch+1, losses))\n",
    "    \n",
    "    return clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tLoss: 0.007728424854576588\n",
      "Epoch: 10\tLoss: 0.005018957424908876\n",
      "Epoch: 15\tLoss: 0.010880048386752605\n",
      "Epoch: 20\tLoss: 0.0028994835447520018\n",
      "Epoch: 25\tLoss: 0.0032447841949760914\n",
      "Epoch: 5\tLoss: 0.002887452719733119\n",
      "Epoch: 10\tLoss: 0.0009570828406140208\n",
      "Epoch: 15\tLoss: 0.0031617318745702505\n",
      "Epoch: 20\tLoss: 0.005867414176464081\n",
      "Epoch: 25\tLoss: 0.0021143059711903334\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    X_train, y_train, \n",
    "    epochs=25, lr=0.05, batch_size=128\n",
    "    )\n",
    "\n",
    "params = training_loop(\n",
    "    X_train, y_train, \n",
    "    epochs=25, lr=0.01, batch_size=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPredicted:  [3 0 6 0 2 7 6 4 1 2 8 8 7 7 9 7 7 3 7 9]\n",
      "\tActual   :  [3 0 6 0 2 7 6 6 1 2 8 8 7 7 4 7 7 3 7 4]\n",
      "\tAccuracy :  0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "s = 20\n",
    "n = np.random.randint(0, len(X_test)-s)\n",
    "output = clf.forward(clf.params, X_test[n:n+s])\n",
    "\n",
    "print(\"\\tPredicted: \", output.argmax(axis=1))\n",
    "print(\"\\tActual   : \", y_test[n:n+s].argmax(axis=1), )\n",
    "print(\"\\tAccuracy : \", (y_test[n:n+s].argmax(axis=1) == output.argmax(axis=1)).sum() / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.73625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "s = 4000\n",
    "n = np.random.randint(0, len(X_test)-s)\n",
    "print(\"Accuracy : {}\".format(\n",
    "    accuracy_score(clf(X_test[n:n+s]).argmax(axis=1), y_test[n:n+s].argmax(axis=1))\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAXlab",
   "language": "python",
   "name": "jaxlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
