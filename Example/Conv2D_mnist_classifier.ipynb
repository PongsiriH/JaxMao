{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jaxmao/JaxMao/Example',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/jaxmao/.local/lib/python3.10/site-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/jaxmao/JaxMao']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jaxmao/JaxMao\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "- Import packages\n",
    "- Import JaxMao\n",
    "- Set seed and key\n",
    "- Import and prepare data (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Jax, MNIST dataset, utils functions and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 42\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import JaxMao functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from jaxmao.Modules import Module\n",
    "from jaxmao.Layers import FC, Conv2D, Flatten\n",
    "from jaxmao.Activations import ReLU, StableSoftmax\n",
    "\n",
    "# Training\n",
    "from jaxmao.Optimizers import GradientDescent\n",
    "from jaxmao.Losses import CategoricalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import and prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "X_train = jnp.array(X_train/255., jnp.float32).reshape(-1, 1, 28, 28)\n",
    "X_test = jnp.array(X_test/255., jnp.float32).reshape(-1, 1, 28, 28)\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "y_train = onehot_encoder.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = onehot_encoder.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our MNIST Classifier\n",
    "\n",
    "We will build simple Convolution-FC-FC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Classifier(Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(1, 32, 3, 2) \n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FC(32*14*14, 32)\n",
    "        self.fc2 = FC(32, 10)\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = StableSoftmax()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model and analyze output behavior\n",
    "- output behavior: shape, value, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "clf = MNIST_Classifier()\n",
    "clf.init_params(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict without and training:** Let see what our model's output look like <br>\n",
    "- The output shape should be (num_inputs, num_classes), where num_classes = 10.\n",
    "- Since our last layer is softmax, sum of each output should be equaled to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  (20, 10)\n",
      "\n",
      "output sum:\n",
      " [1.         1.         0.9999999  0.9999999  1.         1.\n",
      " 0.99999994 1.0000001  1.         1.         0.9999999  1.\n",
      " 1.         1.0000001  0.99999994 0.99999994 1.         0.9999999\n",
      " 0.9999999  1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = X_train[:20]\n",
    "output = clf(x)\n",
    "print(\"output shape: \", output.shape)\n",
    "print()\n",
    "print(\"output sum:\\n\", output.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and jax.grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CategoricalCrossEntropy()\n",
    "\n",
    "def loss_params(params, x, y):\n",
    "    pred = clf.forward(params, x)\n",
    "    return loss(pred, y)\n",
    "\n",
    "grad_loss = jit(value_and_grad(loss_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent training loop\n",
    "Usually, taking gradient on all data points is not possible. <br>\n",
    "We cannot put our entire data into the memory all at once. <br> \n",
    "<br>\n",
    "**Stochastic Gradient Descent:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Psuedo code\n",
    "def training_loop(x, y, epochs, learning_rate, batch_size):\n",
    "    foreach epoch until reach epochs:\n",
    "        x, y = shuffle(x, y)\n",
    "        foreach batch until loop through entire dataset (minus the leftover):\n",
    "            loss, gradient = grad_loss(model.params, batch_x, batch_y)\n",
    "            model.params = optimizer(model.params, gradients, learning_rate)\n",
    "    return model.params\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import sparse\n",
    "\n",
    "optimizer = GradientDescent()\n",
    "def training_loop(x, y, epochs=20, lr=0.01, batch_size=32):\n",
    "    num_batches = len(x) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x, y = shuffle(x, y)\n",
    "        for batch_idx in range(num_batches):\n",
    "            starting_idx = batch_idx * batch_size\n",
    "            ending_idx = (batch_idx + 1) * batch_size\n",
    "            batch_x = x[starting_idx:ending_idx]\n",
    "            batch_y = y[starting_idx:ending_idx].toarray()\n",
    "            # batch_y = sparse.BCOO.from_scipy_sparse(y[starting_idx:ending_idx])\n",
    "            \n",
    "            losses, gradients = grad_loss(clf.params, batch_x, batch_y)\n",
    "            clf.params = optimizer(clf.params, gradients, lr=lr)\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"Epoch: {}\\tbatch loss: {}\".format(epoch+1, losses/batch_size))\n",
    "    \n",
    "    return clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tbatch loss: 0.22568993270397186\n",
      "Epoch: 10\tbatch loss: 0.20340317487716675\n",
      "Epoch: 15\tbatch loss: 0.057628169655799866\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    X_train, y_train, \n",
    "        epochs=15, lr=0.0001, batch_size=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPredicted:  [6 6 4 3 9 8 3 0 1 9 0 5 4 1 9 1 2 7 0 1]\n",
      "\tActual   :  [[6 6 4 3 8 8 3 0 1 9 0 5 4 1 9 1 2 7 0 1]]\n",
      "\tAccuracy :  0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "s = 20\n",
    "n = np.random.randint(0, len(X_test)-s)\n",
    "output = clf.forward(clf.params, X_test[n:n+s])\n",
    "\n",
    "print(\"\\tPredicted: \", output.argmax(axis=1))\n",
    "print(\"\\tActual   : \", y_test[n:n+s].argmax(axis=1).ravel(), )\n",
    "print(\"\\tAccuracy : \", (y_test[n:n+s].toarray().argmax(axis=1) == output.argmax(axis=1)).sum() / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "s = 4000\n",
    "n = np.random.randint(0, len(X_test)-s)\n",
    "print(\"Accuracy : {}\".format(\n",
    "    accuracy_score(clf(X_test[n:n+s]).argmax(axis=1), y_test[n:n+s].toarray().argmax(axis=1))\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAXlab",
   "language": "python",
   "name": "jaxlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
