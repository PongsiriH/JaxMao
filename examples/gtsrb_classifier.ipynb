{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jaxmao/jaxmaov2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, cv2\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def load_GTSRB(color='rgb'):\n",
    "    if not color in ['rgb', 'hsv']:\n",
    "        return False\n",
    "    X_train = load_pickle_file('/home/jaxmao/jaxmao/dataset/GTSRB_hsv/X_train.pkl')\n",
    "    y_train = load_pickle_file('/home/jaxmao/jaxmao/dataset/GTSRB_hsv/y_train.pkl')\n",
    "    X_test = load_pickle_file('/home/jaxmao/jaxmao/dataset/GTSRB_hsv/X_test.pkl')\n",
    "    y_test = load_pickle_file('/home/jaxmao/jaxmao/dataset/GTSRB_hsv/y_test.pkl')\n",
    "\n",
    "    if color == 'rgb':\n",
    "        for i in range(len(X_train)):\n",
    "            X_train[i] = cv2.cvtColor(X_train[i].astype('float32'), cv2.COLOR_HSV2RGB)\n",
    "        for i in range(len(X_test)):\n",
    "            X_test[i] = cv2.cvtColor(X_test[i].astype('float32'), cv2.COLOR_HSV2RGB)\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_GTSRB(color='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start by importing neccessary layers, loss, optimizer, and initializer(optional) <br>\n",
    "initiate jax random key with seed=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1700977349.000391  121493 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "from jaxmaov2.modules import Module, Bind, Conv2d, BatchNorm2d, BatchNorm1d, Dense, GlobalAveragePooling2d, MaxPooling2d\n",
    "from jaxmaov2.optimizers import Adam\n",
    "from jaxmaov2.losses import CategoricalCrossEntropy\n",
    "from jaxmaov2 import initializers as init\n",
    "import jax\n",
    "\n",
    "key = jax.random.key(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define helper class `ConvBNRelu` and the main model `Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool2d = MaxPooling2d(strides=(1,1))\n",
    "\n",
    "class ConvBnRelu(Module):\n",
    "    def __init__(self, in_channels, out_channels, activation=None, strides=1, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2d(in_channels, out_channels, kernel_size=kernel_size, strides=strides, padding='valid', use_bias=False, kernel_init=init.GlorotUniform())\n",
    "        self.bn = BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.bn(self.conv(inputs))\n",
    "        return self.activation(x) if self.activation else x\n",
    "\n",
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0 = BatchNorm2d(3)\n",
    "        self.conv1 = ConvBnRelu(3, 32, jax.nn.relu, 2)\n",
    "        self.conv2 = ConvBnRelu(32, 32, jax.nn.relu, 1)\n",
    "        self.conv3 = ConvBnRelu(32, 64, jax.nn.relu, 2)\n",
    "        self.conv4 = ConvBnRelu(64, 64, jax.nn.relu, 1)\n",
    "        \n",
    "        self.conv_expand = ConvBnRelu(64, 512, jax.nn.relu, 1, 1)\n",
    "        self.conv_out = ConvBnRelu(512, 43, None, strides=1, kernel_size=1)\n",
    "        self.gap = GlobalAveragePooling2d()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = max_pool2d(self.conv2(self.conv1(x)))\n",
    "        x = max_pool2d(self.conv4(self.conv3(x)))\n",
    "        return jax.nn.softmax(self.gap(self.conv_out(self.conv_expand(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize parameters and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()\n",
    "params, states = clf.init(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params, lr=1e-2)\n",
    "cce_loss = CategoricalCrossEntropy(eps=1e-6)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, states, optim_state, X, y):\n",
    "    def apply_loss(params, states, X, y):\n",
    "        prediction, states, reg = clf.apply(X, params, states)\n",
    "        return cce_loss(prediction, y), states\n",
    "    (loss_value, states), gradients = jax.value_and_grad(apply_loss, argnums=0, has_aux=True)(params, states, X, y)\n",
    "    params, optim_state = optimizer.step(params, gradients, optim_state)\n",
    "    return loss_value, params, states, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.5509109497070312\n",
      "1 loss: 0.11460865288972855\n",
      "2 loss: 0.03976920247077942\n",
      "3 loss: 0.023305390030145645\n",
      "4 loss: 0.016676222905516624\n",
      "5 loss: 0.015183232724666595\n",
      "6 loss: 0.003369570942595601\n",
      "7 loss: 0.02264929749071598\n",
      "8 loss: 0.007706231437623501\n",
      "9 loss: 0.0068495688028633595\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "BATCH_NUMS = len(X_train) // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    toltal_losses = 0.0\n",
    "    for batch_idx in range(BATCH_NUMS):\n",
    "        starting_idx = batch_idx*BATCH_SIZE\n",
    "        ending_idx= (batch_idx+1)*BATCH_SIZE\n",
    "        loss_value, params, states, optimizer.states = train_step(params, states, optimizer.states, X_train[starting_idx:ending_idx], y_train[starting_idx:ending_idx])\n",
    "        toltal_losses += loss_value\n",
    "    print('{} loss: {}'.format(epoch, toltal_losses / BATCH_NUMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9715284715284715"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with Bind(clf, params, states) as ctx:\n",
    "    predictions = ctx.module(X_test)\n",
    "    predictions_cls = predictions.argmax(axis=1)\n",
    "    accuracy = accuracy_score(y_test.argmax(axis=1), predictions_cls)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
