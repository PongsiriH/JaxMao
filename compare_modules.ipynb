{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.devices() : [CpuDevice(id=0)]\n",
      "tf.config.list_physical_devices():  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import jaxmao\n",
    "from jaxmao.layers import Conv2D, SimpleDense, Dense, BatchNorm, ReLU, Flatten, StableSoftmax, BatchNorm2D, DepthwiseConv2D, Activation\n",
    "from jaxmao.modules import Module\n",
    "from jaxmao.optimizers import GradientDescent\n",
    "from jaxmao.losses import CategoricalCrossEntropy\n",
    "from jaxmao.metrics import Accuracy, Precision, Recall\n",
    "\n",
    "print('jax.devices() :', jax.devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "print('tf.config.list_physical_devices(): ', tf.config.list_physical_devices())\n",
    "\n",
    "seed = 42\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_enough(A, B, eps=1e-5):\n",
    "    return np.less_equal(np.abs(A - B), eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jax.grad vs tf.GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient: [ 4.  10.8]\n",
      "TensorFlow Gradient: [ 4.  10.8]\n",
      "Are they close enough? [ True  True]\n"
     ]
    }
   ],
   "source": [
    "# Define the function f(x) = x^2\n",
    "def jax_function(x):\n",
    "    return jax.lax.pow(x, 2)\n",
    "\n",
    "def tf_function(x):\n",
    "    return tf.pow(x, 2)\n",
    "\n",
    "input = [2.0, 5.4]\n",
    "\n",
    "# Compute the gradient using JAX\n",
    "jax_grad = jax.grad(jax_function)\n",
    "x_jax = jnp.array(input)\n",
    "grad_jax = jax.vmap(jax_grad)(x_jax)\n",
    "\n",
    "# Compute the gradient using TensorFlow\n",
    "x_tf = tf.Variable(input, dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    y_tf = tf_function(x_tf)\n",
    "grad_tf = tape.gradient(y_tf, x_tf).numpy()\n",
    "\n",
    "# Compare\n",
    "print(\"JAX Gradient:\", grad_jax)\n",
    "print(\"TensorFlow Gradient:\", grad_tf)\n",
    "print(\"Are they close enough?\", np.isclose(grad_jax, grad_tf, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.normal(2, 4, (200, 8, 8, 1)).astype('float32')\n",
    "y_train = np.random.randint(0, 10, (200,)).astype('float32')\n",
    "y_train_enc = np.array(\n",
    "    jax.nn.one_hot(y_train, num_classes=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer                output shape         #'s params           #'s states          \n",
      "bn1                  (4, 8, 8, 1)         2                    0                   \n",
      "conv1                (4, 8, 8, 4)         40                   0                   \n",
      "flatten              (4, 256)             0                    0                   \n",
      "dense1               (4, 128)             32896                0                   \n",
      "dense2               (4, 32)              4128                 0                   \n",
      "dense3               (4, 10)              330                  0                   \n",
      "\n",
      "total parameters: 37396\n",
      "\n",
      "\n",
      "\n",
      "Model: \"keras_denseMNIST\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_30 (Ba  (None, 8, 8, 1)           4         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 8, 8, 4)           40        \n",
      "                                                                 \n",
      " flatten_30 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37398 (146.09 KB)\n",
      "Trainable params: 37396 (146.08 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DenseMNISTClasifier(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add('bn1', jaxmao.layers.BatchNorm2D(1, momentum=0.99, eps=1e-5))\n",
    "        self.add('conv1', jaxmao.layers.Conv2D(1, 4, (3, 3), (1,1), 'relu', weights_initializer=jaxmao.initializers.GlorotNormal()))\n",
    "        self.add('flatten', jaxmao.layers.Flatten())\n",
    "        self.add('dense1', jaxmao.layers.Dense(8*8*4, 128, 'relu', weights_initializer=jaxmao.initializers.GlorotNormal()))\n",
    "        self.add('dense2', jaxmao.layers.Dense(128, 32, 'relu', weights_initializer=jaxmao.initializers.GlorotNormal()))\n",
    "        self.add('dense3', jaxmao.layers.Dense(32, 10, 'softmax', weights_initializer=jaxmao.initializers.GlorotNormal()))\n",
    "    \n",
    "    def forward(self, params, x, state):\n",
    "        x, state = self.apply(params, x, 'bn1', state)\n",
    "        x, state = self.apply(params, x, 'conv1', state)\n",
    "        x, state = self.apply(params, x, 'flatten', state)\n",
    "        x, state = self.apply(params, x, 'dense1', state)\n",
    "        x, state = self.apply(params, x, 'dense2', state)\n",
    "        x, state = self.apply(params, x, 'dense3', state)\n",
    "        return x, state\n",
    "    \n",
    "jaxmao_model = DenseMNISTClasifier()\n",
    "jaxmao_model.init_params(key)\n",
    "summary = jaxmao_model.summarize(input_shape=(4, 8, 8, 1))\n",
    "\n",
    "print('\\n\\n')\n",
    "# Initialize the Sequential model\n",
    "keras_model = keras.Sequential(name='keras_denseMNIST')\n",
    "\n",
    "# Add layers to the keras_model\n",
    "keras_model.add(keras.layers.BatchNormalization(momentum=0.99, input_shape=(8, 8, 1), epsilon=1e-5))\n",
    "keras_model.add(keras.layers.Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "keras_model.add(keras.layers.Flatten())\n",
    "keras_model.add(keras.layers.Dense(units=128, activation='relu'))\n",
    "keras_model.add(keras.layers.Dense(units=32, activation='relu'))\n",
    "keras_model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Summary of the keras_model to show the architecture\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, (jaxmao_layer, keras_layer) in enumerate(zip(jaxmao_model.layers.values(), keras_model.layers)):\n",
    "#     if isinstance(jaxmao_layer, jaxmao.layers.BatchNorm):\n",
    "#         keras_model.layers[index].set_weights(([np.array(jaxmao_layer.params['gamma']), \n",
    "#                                                 np.array(jaxmao_layer.params['beta']), \n",
    "#                                                 np.array(jaxmao_layer.state['running_mean']), \n",
    "#                                                 np.array(jaxmao_layer.state['running_var'])\n",
    "#                                                 ]))\n",
    "#     elif isinstance(jaxmao_layer, jaxmao.layers.Conv2D):\n",
    "#         keras_model.layers[index].set_weights([np.array(value) for value in jaxmao_layer.params['conv2d/simple_conv2d'].values()])\n",
    "#         # TODO if the Conv2D has batchnorm.\n",
    "#     elif isinstance(jaxmao_layer, jaxmao.layers.Flatten):\n",
    "#         pass\n",
    "#     elif isinstance(jaxmao_layer, jaxmao.layers.Dense):\n",
    "#         keras_model.layers[index].set_weights([np.array(value) for value in jaxmao_layer.params['dense/simple_dense'].values()])\n",
    "#         # TODO if the Dense has batchnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 0)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_prediction1 = close_enough(keras_model(X_train).numpy(), jaxmao_model(jaxmao_model.params, X_train), 1e-7)\n",
    "np.prod(close_prediction1.shape), close_prediction1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = jaxmao.losses.CategoricalCrossEntropy('mean_over_batch_size')\n",
    "optimizer = jaxmao.optimizers.GradientDescent(lr=0.01, params=jaxmao_model.params)\n",
    "\n",
    "def _loss_fnx(pure_forward, params, x, y, state):\n",
    "    y_pred, new_state = pure_forward(params, x, state)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    return loss, new_state\n",
    "\n",
    "loss_and_grad = jax.value_and_grad(_loss_fnx, argnums=1, has_aux=True)\n",
    "# (loss, new_state), gradients = jax.block_until_ready(\n",
    "#     loss_and_grad(jaxmao_model.pure_forward, jaxmao_model.params, X_train, y_train_enc, jaxmao_model.state)\n",
    "# )\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4522681"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.losses.CategoricalCrossentropy(reduction='sum_over_batch_size')(y_train_enc, keras_model(X_train)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JaxMao SGD fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss: 0.0010112959425896406, accuracy: 1.0\n",
      "epoch 2: loss: 0.0009999391622841358, accuracy: 1.0\n",
      "epoch 3: loss: 0.0009888584027066827, accuracy: 1.0\n",
      "epoch 4: loss: 0.0009784718276932836, accuracy: 1.0\n",
      "epoch 5: loss: 0.0009669575956650078, accuracy: 1.0\n",
      "epoch 6: loss: 0.0009569003013893962, accuracy: 1.0\n",
      "epoch 7: loss: 0.0009464382892474532, accuracy: 1.0\n",
      "epoch 8: loss: 0.0009363929857499897, accuracy: 1.0\n",
      "epoch 9: loss: 0.0009265993721783161, accuracy: 1.0\n",
      "epoch 10: loss: 0.0009174169972538948, accuracy: 1.0\n",
      "epoch 11: loss: 0.0009073643013834953, accuracy: 1.0\n",
      "epoch 12: loss: 0.0008980348939076066, accuracy: 1.0\n",
      "epoch 13: loss: 0.0008890290046110749, accuracy: 1.0\n",
      "epoch 14: loss: 0.0008798628114163876, accuracy: 1.0\n",
      "epoch 15: loss: 0.0008709843386895955, accuracy: 1.0\n",
      "epoch 16: loss: 0.0008623157627880573, accuracy: 1.0\n",
      "epoch 17: loss: 0.0008538503898307681, accuracy: 1.0\n",
      "epoch 18: loss: 0.0008460630779154599, accuracy: 1.0\n",
      "epoch 19: loss: 0.0008375358884222806, accuracy: 1.0\n",
      "epoch 20: loss: 0.0008291432750411332, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "NUM_BATCHES = len(X_train) // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = 0.0\n",
    "    # we can shuffle training set here\n",
    "    for num_batch in range(NUM_BATCHES):\n",
    "        i_index = num_batch*BATCH_SIZE\n",
    "        e_index = (num_batch+1)*BATCH_SIZE\n",
    "        batch_x = X_train[i_index: e_index]\n",
    "        batch_y = y_train_enc[i_index: e_index]\n",
    "        \n",
    "        (loss, new_state), gradients = loss_and_grad(jaxmao_model.pure_forward, jaxmao_model.params, \n",
    "                                                     batch_x, batch_y, jaxmao_model.state)\n",
    "        jaxmao_model.params, optimizer.state = optimizer(jaxmao_model.params, gradients, optimizer.state)\n",
    "        jaxmao_model.update_state(new_state)\n",
    "        \n",
    "        losses += loss\n",
    "\n",
    "    pred = jaxmao_model(jaxmao_model.params, X_train)    \n",
    "    print('epoch {}: loss: {}, accuracy: {}'.format(epoch+1, losses/NUM_BATCHES, accuracy_score(y_train, pred.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras SGD fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.9615e-04 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.9038e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.8488e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.7929e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.7366e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.6830e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.6286e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.5766e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.5228e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.4700e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "keras_model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), \n",
    "                    loss=keras.losses.CategoricalCrossentropy(reduction='sum_over_batch_size'), \n",
    "                    metrics=['accuracy'])\n",
    "history = keras_model.fit(X_train, y_train_enc, epochs=20, batch_size=10, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
